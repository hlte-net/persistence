#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
const util = require('util');
const { Client, request } = require('undici');
const pkg = require('./package.json');
const docopt = require('@eyalsh/docopt').default;
const Redis = require('ioredis');
const mustache = require('mustache');
const probeImageSize = require('probe-image-size');
const sqlite = require('sqlite3');

const Defaults = Object.freeze({
  dataPath: './data',
  mainDbPath: process.env.HLTE_DB_PATH,
  redisEnvVarName: 'HLTE_REDIS_URL',
  cacheKeySuffix: pkg.name + pkg.version,
  gallery: {
    templateFile: './gallery.html',
    livePath: process.env.HLTE_GALLERY_LIVE_PATH,
    requiredPixels: 200000
  }
});

const WorkerPdfOptsJsonB64 = btoa(JSON.stringify({
  displayHeaderFooter: true,
  footerTemplate: "<i class='title'></i> &lt;<span class='url'></span>&gt; at <span class='date'></span>, " +
    "page <span class='pageNumber'></span> of <span class='totalPages'></span>"
}));

class InternalError extends Error { }

const doc = `
Hilite media persistence worker.

Usage:
  ${path.parse(__filename).base} <streamKey> [--data=path] [--redis=url] [--interval=mins] [--keep] [--debug]

Required:
  streamKey         The stream key to check for work.

Optional:
  --data=path       Data storage path. [default: ${Defaults.dataPath}]
  --redis=url       Use 'url' as the Redis connection URL. If not given, env var ${Defaults.redisEnvVarName} is [default: ${process.env[Defaults.redisEnvVarName]}]
  --interval=mins   Enables daemon mode and sets how long between each wakeup.
  --keep            Does not remove work items from the queue upon successful completion.
  --debug           Enable debug logging (can also be enabled by setting env var DEBUG=1).
`;

async function processPrimaryImage(imageUrl, imagePath, metadata) {
  if (!Defaults.gallery) {
    return;
  }

  console.log(`Processing primary image for gallery ${imageUrl} ${imagePath}`, metadata);
  const imageData = await probeImageSize(imageUrl);
  console.log('Image dimensions', imageData);

  if (!imageData || (imageData.width * imageData.height < Defaults.gallery.requiredPixels)) {
    return;
  }

  const db = new sqlite.Database(Defaults.mainDbPath, sqlite.OPEN_READONLY);
  const [{ annotation }] = await new Promise((resolve, reject) => {
    console.log('looking for ', metadata.entryMap);
    db.all('SELECT * FROM hlte WHERE checksum = ? AND timestamp = ?',
      [metadata.entryMap.checksum, metadata.entryMap.timestamp],
      (err, rows) => {
        db.close();

        if (err) {
          return reject(err);
        }

        if (rows.length > 1) {
          return reject(new Error(`More than one row fetched for checksum=${metadata.entryMap.checksum} AND timestamp=${metadata.entryMap.timestamp}`));
        }

        console.log('ROWS', rows);
        resolve(rows);
      });
  });

  if (annotation.indexOf('#publish') === -1) {
    console.log(`Skipping image because '#publish' not included in annotation: "${annotation}"`);
    return;
  }

  const tmpl = (await fs.promises.readFile(Defaults.gallery.templateFile)).toString('utf8');
  return fs.promises.writeFile(Defaults.gallery.livePath, mustache.render(tmpl, {
    imageUrl,
    saveTime: new Date(metadata.entryMap.timestamp / 1e6).toLocaleString(),
    linkUrl: metadata.entryMap.secondaryURI
  }));
}

const loggerInit = (function () {
  const LEVELS = ['info', 'log', 'warn', 'error', 'debug'];
  const enabledLevels = new Set(LEVELS);
  const [NAME, VERSION] = [pkg.name, pkg.version];

  function enableLevel(l) {
    enabledLevels.add(l.toLowerCase());
  }

  function disableLevel(l) {
    enabledLevels.delete(l.toLowerCase());
  }

  let logger;

  function initialize(args, overrideConsole = true) {
    if (!logger) {
      const APP_NAMEVER = [NAME, VERSION, process.pid].join('/');
      const dsMapper = (dsArr, colors = false) => dsArr.map(x => typeof x === 'string' ? x : util.inspect(x, { colors, depth: null })).join(' ');

      const _outStream = fs.createWriteStream(`${NAME}.log`);
      const _console = LEVELS.reduce((a, x) => ({ [x]: console[x], ...a }), {});

      const _emit = (level, ...a) => {
        if (!enabledLevels.has(level)) {
          return;
        }

        const dstrArr = [new Date(), `<${APP_NAMEVER}/${level}>`, ...a];
        _console[level](dsMapper(dstrArr, true));
        _outStream.write(dsMapper([{ level }, ...dstrArr, '\n']));
      };

      logger = LEVELS.reduce((a, lvl) => ({ [lvl]: _emit.bind(logger, lvl), ...a }), { APP_NAMEVER });

      if (overrideConsole) {
        LEVELS.forEach(level => (console[level] = logger[level]));
      }

      if (!process.env.DEBUG && !args.debug) {
        disableLevel('debug');
      }
    }

    return logger;
  }

  initialize.enableLevel = enableLevel;
  initialize.disableLevel = disableLevel;

  return initialize;
})();

async function processOneWork(args) {
  const redisExecOne = async (cmdFuncName, ...a) => {
    const redisClient = new Redis(args.redis);
    const retVal = await redisClient[cmdFuncName](...a);
    redisClient.disconnect();
    return retVal;
  };

  const workList = await redisExecOne('xread', 'count', '1', 'streams', args.streamKey, 0);

  if (!workList || !workList.length === 1) {
    return null;
  }

  if (workList[0][0] !== args.streamKey || workList[0][1].length !== 1 || workList[0][1][0].length !== 2) {
    console.error('Malformed work unit', workList);
    return null;
  }

  const [entryId, kvList] = workList[0][1][0];
  const entryMap = kvList.reduce((b, ele, i) => {
    if (!(i % 2)) {
      b[ele] = kvList[i + 1];
    }

    return b;
  }, {});
  console.debug('workList', workList, '-> entryMap', entryMap);

  const { checksum, timestamp } = entryMap;
  const longestKey = Object.keys(entryMap).reduce((a, i) => Math.max(a, i.length), -2e32);
  console.log(`Processing entry ID ${entryId} with the following fields:`);
  Object.entries(entryMap).forEach(([k, v]) => console.log(`\t${k.padEnd(longestKey + 1)}->  ${v}`));

  const writeStreamForIdType = (idType, headers, assocType) => {
    const idPath = path.resolve(args.data, idType);
    fs.mkdirSync(idPath, { recursive: true });
    const fExtn = assocType ? 'json' : headers['content-type'].split(';')[0].split('/')[1];
    const resPath = path.resolve(idPath, `${checksum}-${timestamp}${assocType ? `.${assocType}` : ''}.${fExtn}`);
    return fs.createWriteStream(resPath);
  };

  const processSingleURI = async (u, i) => {
    const idType = i === 0 ? 'primary' : 'secondary';
    if (u.length) {
      // fire off a PP worker persistor right away
      if (idType === 'primary') {
        const params = new URLSearchParams([
          ['hlteUuid', `${checksum}-${timestamp}`],
          ['pdfOpts', WorkerPdfOptsJsonB64],
          ['url', u],
        ])
        const workerReq = `${process.env.X_HLTE_PERSIST_URL}/?${params.toString()}`;
        console.log('Worker request:', workerReq);
        request(workerReq, {
          headers: {
            'x-hlte-token': process.env.X_HLTE_TOKEN
          }
        })
          .then((resp) => {
            console.log(`Persist worker result`, resp.statusCode);
            resp.body.json().then(console.log).catch(console.error);
          })
          .catch(console.error);
      }

      const url = new URL(u);
      console.debug(`GETing ${url.origin}/${url.pathname + url.search} (${idType})...`);
      return (new Client(url.origin)).stream({
        path: url.pathname + url.search,
        method: 'GET'
      }, ({ statusCode, headers }) => {
        console.debug(`GET ${url.origin}/${url.pathname + url.search}: ${statusCode}`, headers);
        if (statusCode !== 200) {
          console.error(`Request to ${u} failed with ${statusCode}; skipping`);
          console.debug(entryMap, url, statusCode, headers);
          return null;
        }

        console.log(`Processing ${idType}: content-type is "${headers['content-type']}"...`);

        const metadata = {
          entryMap,
          headers,
          processed: new Date()
        };
        const metaWs = writeStreamForIdType('metadata', null, idType);
        metaWs.write(JSON.stringify(metadata, null, 2));
        console.log('Wrote metadata into:');
        console.log('\t' + metaWs.path.replace(args.data, ''));
        metaWs.close();

        const ws = writeStreamForIdType(idType, headers);
        console.log('Piped resource contents into:');
        console.log('\t' + ws.path.replace(args.data, ''));

        if (idType === 'primary' && headers['content-type'].startsWith('image/')) {
          processPrimaryImage(u, ws.path, metadata)
            .catch((e) => console.error('processPrimaryImage failed', e, u, ws.path, metadata));
        }

        return ws;
      });
    }

    return null;
  };

  try {
    (await Promise.all([entryMap.primaryURI, entryMap.secondaryURI].map(processSingleURI)));
  } catch (err) {
    console.error(`Processing of ${entryId} failed! Full error:`);
    console.error(err);

    const deadKey = [args.streamKey, 'failed'].join(':');
    const deadEntryId = await redisExecOne('xadd', deadKey, entryId, 'checksum', checksum, 'timestamp', timestamp);
    console.error(`Recorded failure as ID ${deadEntryId} in ${deadKey}`);
  }

  if (!args.keep) {
    const rmRet = await redisExecOne('xdel', args.streamKey, entryId);
    console.log(`Removed ${args.streamKey} entry ${entryId}?`, rmRet);
  }

  console.log();
  return entryId;
}

try {
  const args = Object.entries(docopt(doc)).reduce((a, [k, v]) => ({ [k.replace(/[<>-]+/g, '')]: v, ...a }), {});
  args.data = path.resolve(args.data);

  loggerInit(args);
  console.debug(args);

  try {
    fs.mkdirSync(path.resolve(args.data));
  } catch (err) {
    if (err.code !== 'EEXIST') {
      throw err;
    }
  }

  const mainBound = processOneWork.bind(null, args);
  let mainExec = mainBound;

  if (args.interval) {
    const go = () => {
      mainBound().then((processedSomeWork) => setTimeout(go, processedSomeWork ? 0 : args.interval * 60 * 1000));
    };

    mainExec = go;
    process.on('SIGINT', () => process.exit(0));
    console.info(`Daemonizing, will wake up every ${args.interval} minute(s)...`);
  }

  mainExec();
} catch (err) {
  if (err instanceof InternalError) {
    console.error(err);
  } else {
    console.log(err.message);
  }
}
